---
title: "Homework 3"
author: "Yaa Ababio"
output: github_document
---

```{r setup}
library(tidyverse)
library(hexbin)
library(ggridges)
library(patchwork)
library(p8105.datasets)

knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


### Problem 1 

#### Describing the Instacart dataset
The `instacart` dataset contains information about users, types of products, and purchase dates/times of items available on the online grocery service Instacart. This data set is comprised of  `r nrow(instacart)` rows and `r ncol(instacart)` columns.here are `r nrow (distinct(instacart, user_id))` distinct users, and each row in the dataset represents a product from a order.



#### How many aisles, and which are most items ordered from?
```{r aisles}
aisles = 
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))

```
There are `r ncol(aisles)` and the top 3 aisles with the most items ordered from them are:

*fresh vegetables - 150,609

*fresh fruits - 150,473

*packaged vegetables fruits - 78,493




#### Creating a plot with number of items (n> 10,000) ordered in each aisle. 

```{r instacart_plot}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```




#### Creating a table showing the 3 most popular items in the "baking ingredients", "dog food care", and "packaged vegetables fruits" aisles. 
```{r popular_table}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```




#### Creating a table showing the  mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r apple_coffe_hour}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	) %>%
  knitr::kable()
```






### Problem 2 

#### Load, tidy, and describe the accelerometer data set. 
```{r tidy_accel}
accel_df = read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count"
    ) %>%
  mutate(
    minute = as.numeric(minute),
    day = as.factor(day),
    day = fct_relevel(day, "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"),
    weekend = as.numeric(day %in% c("Saturday", "Sunday")),
    day_type = factor(recode(weekend, '1' = "weekend", '0' = "weekday"))) %>%
  select(-weekend)
    
  
    
  
```
The Accelerometer dataset contains five weeks of accelerometer data collected from a 63 year old male CUMC patient with congestive heart failure. This dataset is comprised of `r nrow(accel_df)` observations and `r ncol(accel_df)` variables. Variables include: `r names(accel_df)`.The tidy dataset specifies whether a day is a weekday or weekend via the day_id variable, and has collapsed the activity count into a more easily understandable format. The mean activity count for this dataset is `r round(mean(pull(accel_df,activity_count)),2)` with a standard deviation of `r round(sd(pull(accel_df, activity_count)),2)`.


#### Creating total activity per day variable and associated table.
```{r activity_per_day}
activity_per_day =
accel_df %>%
  group_by(day, week) %>%
  summarize(total_activity = sum(activity_count)) %>%
  pivot_wider(
    names_from = day,
    values_from = total_activity)
 
activity_per_day %>%
  knitr::kable()

```
## Description of table trends


#### Creating a single-panel plot that shows the 24-hour activity time courses for each day.
```{r accel_plot}

accel_df %>%
  ggplot(aes(x = minute, y = activity_count, group = day_id, color = day)) + 
  geom_line(aes(group = day), alpha = 0.2) +
  geom_smooth(aes(group = day), se = FALSE) +
  labs(
    title = "24 hr Activity Time Courses"
    ) + 
  theme(plot.title = element_text(hjust = 0.5))
  
 
  


```


## Description of plot trends



### Problem 3

```{r setup_2, include = FALSE}
data("ny_noaa")
```

#### Exploring and describing the NY NOAA dataset.

The `ny_noaa` dataset consists of `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. 


##### Cleaning the NOAA dataset by separating the date variable and ensuring reasonable measurement units.
```{r}
ny_noaa_clean = 
 ny_noaa %>%
 separate(date, into = c("year", "month", "day"), convert = TRUE) %>%
 mutate(
   prcp = prcp/10,
   year = as.numeric(year),
   month = as.numeric(month),
   day = as.numeric(day),
   tmax = as.numeric(tmax)/10,
   tmin = as.numeric(tmin)/10
 )
```

#### Determining the most comonly observed snowfall value. 
```{r}

ny_noaa_clean %>%
 count(snow, name = "n_obs") %>%
 mutate(rank = min_rank(n_obs)) %>%
 arrange(desc(rank))


```
The most commonly observed snowfall value (i.e. observation with the largest rank number) is 0 degrees Celscius. This makes sense, because for most of the calendar year it is not snowing at all. 

#### Creating a two-panel plot for average max temperature in January and July.
```{r}
 
ny_noaa_clean %>%
filter(month %in% c("1", "7")) %>%
mutate(month = month.name[month]) %>%
group_by(id, year, month) %>%
summarize(mean_tmax = mean(tmax, na.rm = TRUE)) %>%
ggplot(aes(x = year, y = mean_tmax, group = id)) +
  geom_line() +
  geom_path() +
  facet_grid(~ month) +
  labs(title = "Average Temp for January and July", x = "Year", y = "Average Maximum Temp (°C)") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + 
  theme(legend.position = "none") +
  theme(plot.title = element_text(hjust = 0.5))


```

There is a clear, interpretable structure. As expected, the average maximum temperature for January is much lower than that of July between the years 1980 and 2010. The average maximum temperature for January ranges from approximately -10 °C to 10 °C, whereas the average maximum temperature from approximately 20°C to 30°C. There are appears to be an outliers present in the data. The outliers for January tend to be above average temperatures, whereas the outliers for July tend to be below average temperatures.

#### Creating a two-panel plot showing (i) tmax vs. tmin and the (ii) distribution of snowfall values.
```{r}
tmaxmin =
  ny_noaa_clean %>%
  drop_na(tmax, tmin) %>%
  ggplot(aes(x = tmin, y = tmax)) +
  labs(
    title = "Temperature (max vs. min) Hexplot",
    x = "Minimum Daily Temperature (C)",
    y = "Maximum Daily Temperature (C)") +
  geom_hex() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.text = element_text(hjust = 1)) +
  theme(legend.position = "right") +
  theme(legend.box = "horizontal")
  

snow_dist = 
  ny_noaa_clean %>%
  drop_na(snow) %>%
  filter(snow > 0 & snow < 100) %>%
  ggplot(aes(x = year, y = snow)) +
  labs(
    title = "Snowfall Values Boxplot",
    x = "Year",
    y = "Snowfall (mm)") +
  geom_boxplot(aes(group = year)) +
  theme(plot.title = element_text(hjust = 0.5))
  
     
tmaxmin + snow_dist
  

```

