---
title: "Homework 3"
author: "Yaa Ababio"
output: github_document
---

```{r setup}
library(tidyverse)
library(hexbin)
library(ggridges)
library(patchwork)
library(p8105.datasets)

knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


### Problem 1 

#### Describing the Instacart dataset
The `instacart` dataset contains information about users, types of products, and purchase dates/times of items available on the online grocery service Instacart. This data set is comprised of  `r nrow(instacart)` rows and `r ncol(instacart)` columns. There are `r nrow (distinct(instacart, user_id))` distinct users, and each row in the dataset represents a product from a order.



#### How many aisles, and which are most items ordered from?
```{r aisles}
aisles = 
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))

```
There are `r nrow(aisles)` aisles and the top 3 aisles with the most items ordered from them are:

*fresh vegetables - 150,609

*fresh fruits - 150,473

*packaged vegetables fruits - 78,493




#### Creating a plot with number of items (n> 10,000) ordered in each aisle. 

```{r instacart_plot}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```




#### Creating a table showing the 3 most popular items in the "baking ingredients", "dog food care", and "packaged vegetables fruits" aisles. 
```{r popular_table}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```




#### Creating a table showing the  mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r apple_coffe_hour}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	) %>%
  knitr::kable()
```






### Problem 2 

#### Load, tidy, and describe the accelerometer data set. 
```{r tidy_accel}
accel_df = read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count"
    ) %>%
  mutate(
    minute = as.numeric(minute),
    day = as.factor(day),
    day = fct_relevel(day, "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"),
    weekend = as.numeric(day %in% c("Saturday", "Sunday")),
    day_type = factor(recode(weekend, '1' = "weekend", '0' = "weekday"))) %>%
  select(-weekend)
    
  
    
  
```
The Accelerometer dataset contains 5 weeks of accelerometer data collected from a 63 year old male CUMC patient with congestive heart failure. This dataset is comprised of `r nrow(accel_df)` observations and `r ncol(accel_df)` variables. Variables include: `r names(accel_df)`. The activity count in this dataset ranges from `r summarize(accel_df, min(activity_count))` to `r summarize(accel_df, max(activity_count))`. The tidy dataset specifies whether a day is a weekday or weekend via the day_type variable, and has collapsed the activity count into a more easily understandable format.


#### Creating total activity per day variable and associated table. Describing trends within the table.
```{r activity_per_day}
activity_per_day =
accel_df %>%
  group_by(day, week) %>%
  summarize(total_activity = sum(activity_count)) %>%
  pivot_wider(
    names_from = day,
    values_from = total_activity)
 
activity_per_day %>%
  knitr::kable()

```

The highest amount of activity appears on Monday during week 3, and the lowest amount of activity appears to occur on Saturday during weeks 4 and 5. Upon further assessment of the low activity counts for Saturday week 4 & 5, it appears that activity count was recorded as '1' for every minute during each of those days (total of 1440 minutes per day). This may be a potential device measurement error.


#### Creating a single-panel plot that shows the 24-hour activity time courses for each day.
```{r accel_plot}

accel_df %>%
  ggplot(aes(x = minute, y = activity_count, group = day_id, color = day)) + 
  geom_line(aes(group = day), alpha = 0.2) +
  geom_smooth(aes(group = day), se = FALSE) +
  labs(
    title = "24 hr Activity Time Courses"
    ) + 
  theme(plot.title = element_text(hjust = 0.5))
  
 
  


```


The plot suggests that activity on Fridays tends to be highest at the end of the day, whereas activity on Sundays tends to be highest in the middle of the day. Activity on Saturdays is relatively consistent throughout the day, and dips towards the end of the day. Because of the nature of the conflating lines of this plot, it is difficult to discern more information and further analysis or differing visual representation may be needed. 



### Problem 3

```{r setup_2, include = FALSE}
data("ny_noaa")
```

#### Exploring and describing the NY NOAA dataset.

The `ny_noaa` dataset describes weather information collected from New York state weather stations from January 1st, 1981 to December 31st, 2010. It consists of `r nrow(ny_noaa)` rows , `r ncol(ny_noaa)` columns, and the following variables: `r names(ny_noaa)`. These variables describe the unique weather station ID, date of observation, precipitation, snowfall, snow depth, maximum temperature, and minimum temperature. 

Information regarding missing values in the dataset is as follows: 

* total `NA`s in df: `r sum(is.na(ny_noaa))`

* `prcp`:`r sum(is.na(pull(ny_noaa, prcp)))` `NA`s

* `snow`: `r sum(is.na(pull(ny_noaa, snow)))` `NA`s

* `snwd`: `r sum(is.na(pull(ny_noaa, snwd)))` `NA`s

* `tmax`: `r sum(is.na(pull(ny_noaa, tmax)))` `NA`s

* `tmin`: `r sum(is.na(pull(ny_noaa, tmin)))` `NA`s

There appear to be a significant number of missing values in the above mentioned variables. These may need to dropped from the dataset during the data visualization process.

##### Cleaning the NOAA dataset by separating the date variable and ensuring reasonable measurement units.
```{r}
ny_noaa_clean = 
 ny_noaa %>%
 separate(date, into = c("year", "month", "day"), convert = TRUE) %>%
 mutate(
   prcp = prcp/10,
   year = as.numeric(year),
   month = as.numeric(month),
   day = as.numeric(day),
   tmax = as.numeric(tmax)/10,
   tmin = as.numeric(tmin)/10
 )
```
Note that precipitation (`prcp`) has been converted from tenths of mm to mm to allow for uniformity (in comparison to snowfall and snow depth, which are also measured in mm). Additionally, temperature (`tmin` and `tmax`) have been converted from tenths of degrees Celsius to degrees Celsius to make the data more readily interpretable.

#### Determining the most comonly observed snowfall value. 
```{r}

ny_noaa_clean %>%
 count(snow, name = "n_obs") %>%
 mutate(rank = min_rank(n_obs)) %>%
 arrange(desc(rank))


```
The most commonly observed snowfall value (i.e. observation with the largest rank number) is 0 mm. This makes sense, because for most of the calendar year it is not snowing at all. 

#### Creating a two-panel plot for average max temperature in January and July.
```{r}
 
ny_noaa_clean %>%
filter(month %in% c("1", "7")) %>%
mutate(month = month.name[month]) %>%
group_by(id, year, month) %>%
summarize(mean_tmax = mean(tmax, na.rm = TRUE)) %>%
ggplot(aes(x = year, y = mean_tmax, group = id)) +
  geom_line() +
  geom_path() +
  facet_grid(~ month) +
  labs(title = "Average Temp for January and July", x = "Year", y = "Average Maximum Temp (°C)") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + 
  theme(legend.position = "none") +
  theme(plot.title = element_text(hjust = 0.5))


```

There is a clear, interpretable structure. As expected, the average maximum temperature for January is much lower than that of July between the years 1980 and 2010. The average maximum temperature for January ranges from approximately -10 °C to 10 °C, whereas the average maximum temperature from approximately 20°C to 30°C. There appear to be outliers present in the data. The outliers for January tend to be above average temperatures, whereas the outliers for July tend to be below average temperatures.

#### Creating a two-panel plot showing (i) tmax vs. tmin and the (ii) distribution of snowfall values.
```{r}
tmaxmin =
  ny_noaa_clean %>%
  drop_na(tmax, tmin) %>%
  ggplot(aes(x = tmin, y = tmax)) +
  labs(
    title = "Temperature (max vs. min) Hexplot",
    x = "Minimum Daily Temperature (C)",
    y = "Maximum Daily Temperature (C)") +
  geom_hex() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.text = element_text(hjust = 1)) +
  theme(legend.position = "right") +
  theme(legend.box = "horizontal")
  

snow_dist = 
  ny_noaa_clean %>%
  drop_na(snow) %>%
  filter(snow > 0 & snow < 100) %>%
  ggplot(aes(x = year, y = snow)) +
  labs(
    title = "Snowfall Values Boxplot",
    x = "Year",
    y = "Snowfall (mm)") +
  geom_boxplot(aes(group = year)) +
  theme(plot.title = element_text(hjust = 0.5))
  
     
tmaxmin + snow_dist
  

```

